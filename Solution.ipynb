{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnalystChidinma/anything_data/blob/main/Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0XrtrMz0_ev"
      },
      "source": [
        "\"\"\"\n",
        "Scenario:\n",
        "\n",
        "You're building a monitoring system for a data pipeline in a data lakehouse architecture.\n",
        "Your job is to analyze the metadata logs from multiple data sources and determine the\n",
        "health status of each pipeline run.\n",
        "\n",
        "Each pipeline run is logged in a JSON-like format that will be provided below.\n",
        "\n",
        "You are given a list of such logs for different pipelines. Your task is to analyze each log and apply the\n",
        "following complex conditional rules to determine the pipeline health status:\n",
        "\n",
        "âœ… Evaluation Rules:\n",
        "\n",
        "Assign a health_status field with one of the following values:\n",
        "\n",
        "    - \"HEALTHY\"\n",
        "    - \"WARNING\"\n",
        "    - \"CRITICAL\"\n",
        "\n",
        "Based on the following logic:\n",
        "\n",
        "1. HEALTHY if:\n",
        "\n",
        "    - status_code is 200 AND\n",
        "    - errors is empty AND\n",
        "    - warnings is empty or only includes \"late data arrival\" AND\n",
        "    - duration_seconds is less than 600 AND\n",
        "    - max_latency_seconds is less than 10\n",
        "\n",
        "2. WARNING if any of the following:\n",
        "\n",
        "    - status_code is 200 AND\n",
        "        - duration_seconds is between 600 and 1200 OR\n",
        "        - max_latency_seconds is between 10 and 30 OR\n",
        "        - warnings contains non-late data warning messages\n",
        "    - OR there are fewer than 100 records ingested (record_count < 100) but no errors\n",
        "\n",
        "3. CRITICAL if:\n",
        "\n",
        "    - status_code is not 200\n",
        "    - OR there are one or more errors\n",
        "    - OR duration_seconds > 1200\n",
        "    - OR max_latency_seconds > 30\n",
        "    - OR record_count == 0\n",
        "\n",
        "ðŸŽ¯ Your Task:\n",
        "\n",
        "1. Write a function evaluate_pipeline_health(log) that takes a single log dictionary and returns the same dictionary with a new key health_status assigned based on the above rules.\n",
        "\n",
        "2. Write a function evaluate_all_pipelines(logs: List[Dict]) -> List[Dict] to apply this to a list of logs.\n",
        "\n",
        "3. Print a summary:\n",
        "\n",
        "    - Total pipelines evaluated\n",
        "\n",
        "    - Count of each health status category\n",
        "\n",
        "ðŸ§ª Bonus Challenge (optional):\n",
        "\n",
        "    - Add a rule: if the ingestion time is between midnight and 4 AM UTC, and the pipeline is \"CRITICAL\",\n",
        "    mark it for \"High Priority Alert\".\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example logs for testing\n",
        "pipeline_log = {\n",
        "    \"pipeline_name\": \"user_event_ingestion\",\n",
        "    \"status_code\": 200,\n",
        "    \"errors\": [],\n",
        "    \"warnings\": [\"late data arrival\"],\n",
        "    \"duration_seconds\": 452,\n",
        "    \"max_latency_seconds\": 5.6,\n",
        "    \"record_count\": 124500,\n",
        "    \"ingestion_time_utc\": \"2025-10-22T05:28:00Z\",\n",
        "    \"source\" : \"kafka\"\n",
        "}\n",
        "\n",
        "\n",
        "# list of logs like above Example\n",
        "logs = [\n",
        "    {\n",
        "        \"pipeline_name\" : \"user_event_ingestion\",\n",
        "    \"status_code\" : 200,\n",
        "    \"errors\" : [],\n",
        "    \"warnings\" : [\"late data arrival\"],\n",
        "    \"duration_seconds\" : 452,\n",
        "    \"max_latency_seconds\" : 5.6,\n",
        "    \"record_count\" : 124500,\n",
        "    \"ingestion_time_utc\" : \"2025-10-22T05:28:00Z\",\n",
        "    \"source\" : \"kafka\"\n",
        "    },\n",
        "    {\n",
        "        \"pipeline_name\" : \"transaction_data_load\",\n",
        "    \"status_code\" : 500,\n",
        "    \"errors\" : [\"Database connection timeout\"],\n",
        "    \"warnings\" : [],\n",
        "    \"duration_seconds\" : 1300,\n",
        "    \"max_latency_seconds\" : 45.2,\n",
        "    \"record_count\" : 0,\n",
        "    \"ingestion_time_utc\" : \"2025-10-22T14:15:00Z\",\n",
        "    \"source\" : \"s3\"\n",
        "    },\n",
        "    {\n",
        "\n",
        "        \"pipeline_name\": \"product_catalog_sync\",\n",
        "        \"status_code\": 200,\n",
        "        \"duration_seconds\": 800,\n",
        "        \"record_count\": 80,\n",
        "        \"max_latency_seconds\": 15.0,\n",
        "        \"errors\": [],\n",
        "        \"warnings\": [\"schema mismatch\"],\n",
        "        \"ingestion_time\": \"2025-10-08T09:00:00Z\",\n",
        "        \"source\": \"api\"\n",
        "    },\n",
        "    {\n",
        "        \"pipeline_name\": \"inventory_update\",\n",
        "        \"status_code\": 200,\n",
        "        \"duration_seconds\": 300,\n",
        "        \"record_count\": 1500,\n",
        "        \"max_latency_seconds\": 8.0,\n",
        "        \"errors\": [],\n",
        "        \"warnings\": [],\n",
        "        \"ingestion_time\": \"2025-10-08T03:45:00Z\",\n",
        "        \"source\": \"ftp\"\n",
        "    }\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "XhcjNb2PGMUw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***QUESTION 1***\n",
        "\n",
        "Write a function evaluate_pipeline_health(log) that takes a single log dictionary and returns the same dictionary with a new key health_status assigned based on the above rules."
      ],
      "metadata": {
        "id": "bYI-ukdfsyc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8I5tdk-F4A-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c337c222-9289-4812-f515-e3eb7b74d59c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pipeline_name': 'user_events_ingestion',\n",
              " 'status_code': 200,\n",
              " 'duration_seconds': 452,\n",
              " 'record_count': 124500,\n",
              " 'max_latency_seconds': 5.6,\n",
              " 'errors': [],\n",
              " 'warnings': ['late data arrival'],\n",
              " 'ingestion_time': '2025-10-08T02:30:00Z',\n",
              " 'source': 'kafka',\n",
              " 'health_status': 'HEALTHY'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# healthy_satus condition\n",
        "def evaluate_pipeline_health(log):\n",
        "    if (\n",
        "        log[\"status_code\"] == 200\n",
        "        and log[\"errors\"] == []\n",
        "        and (log[\"warnings\"] == [] or log[\"warnings\"] == [\"late data arrival\"])\n",
        "        and log[\"duration_seconds\"] < 600\n",
        "        and log[\"max_latency_seconds\"] < 10\n",
        "\n",
        "      )  :\n",
        "        log.update({\"health_status\": \"HEALTHY\"})\n",
        "\n",
        "#warning\n",
        "\n",
        "    elif (\n",
        "         log[\"status_code\"] == 200\n",
        "        and (log[\"duration_seconds\"] >= 600 and log[\"duration_seconds\"] <= 1200)\n",
        "        or (log[\"max_latency_seconds\"] >= 10 and log[\"max_latency_seconds\"] <= 30)\n",
        "        or log[\"warnings\"] != [\"late data arrival\"]\n",
        "        or log[\"record_count\"] < 100 and log[\"errors\"] == []\n",
        "\n",
        "        ):\n",
        "        log.update({\"health_status\": \"WARNING\"})\n",
        "\n",
        "#Critical\n",
        "\n",
        "    elif (\n",
        "        log[\"status_code\"] != 200 or [\"errors\"] != []\n",
        "        or log[\"duration_seconds\"] > 1200\n",
        "        or log[\"max_latency_seconds\"] > 30\n",
        "        or log[\"record_count\"] == 0\n",
        "    ):\n",
        "        log.update({\"health_status\": \"CRITICAL\"})\n",
        "\n",
        "\n",
        "#unknown\n",
        "\n",
        "    else:\n",
        "        log.update({\"health_status\": \"UNKNOWN\"})\n",
        "    return log\n",
        "\n",
        "evaluate_pipeline_health(pipeline_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Oh7dfH9_tjr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***QUESTION 2***\n",
        "\n",
        "Write a function evaluate_all_pipelines(logs: List[Dict]) -> List[Dict] to apply this to a list of logs.\n",
        "\n"
      ],
      "metadata": {
        "id": "iFjLK-lGtP-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate all pipelines logs\n",
        "\n",
        "health_evaluation_logs = []\n",
        "\n",
        "def evaluate_all_pipelines(logs):\n",
        "\n",
        "  for log in logs:\n",
        "    # Apply HEALTHY condition\n",
        "    if (\n",
        "        log[\"status_code\"] == 200\n",
        "        and log[\"errors\"] == []\n",
        "        and (log[\"warnings\"] == [] or log[\"warnings\"] == [\"late data arrival\"])\n",
        "        and log[\"duration_seconds\"] < 600\n",
        "        and log[\"max_latency_seconds\"] < 10\n",
        "      ):\n",
        "        log[\"health_status\"] = \"HEALTHY\"\n",
        "\n",
        "    # Apply WARNING condition\n",
        "    elif (\n",
        "         log[\"status_code\"] == 200\n",
        "        and (\n",
        "            (log[\"duration_seconds\"] >= 600 and log[\"duration_seconds\"] <= 1200)\n",
        "            or (log[\"max_latency_seconds\"] >= 10 and log[\"max_latency_seconds\"] <= 30)\n",
        "            or (log[\"warnings\"] != [] and log[\"warnings\"] != [\"late data arrival\"])\n",
        "        )\n",
        "    ) or (log[\"record_count\"] < 100 and log[\"errors\"] == []):\n",
        "        log[\"health_status\"] = \"WARNING\"\n",
        "\n",
        "    # Apply CRITICAL condition\n",
        "    else:\n",
        "        log[\"health_status\"] = \"CRITICAL\"\n",
        "\n",
        "\n",
        "    health_evaluation_logs.append(log)\n",
        "\n",
        "  return health_evaluation_logs\n",
        "\n",
        "evaluated_logs = evaluate_all_pipelines(logs)\n",
        "print(evaluated_logs)"
      ],
      "metadata": {
        "id": "gezj7nY5WdWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebd397d-0acb-4bff-bb7d-714c922a0b9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'pipeline_name': 'user_event_ingestion', 'status_code': 200, 'errors': [], 'warnings': ['late data arrival'], 'duration_seconds': 452, 'max_latency_seconds': 5.6, 'record_count': 124500, 'ingestion_time_utc': '2025-10-22T05:28:00Z', 'source': 'kafka', 'health_status': 'HEALTHY'}, {'pipeline_name': 'transaction_data_load', 'status_code': 500, 'errors': ['Database connection timeout'], 'warnings': [], 'duration_seconds': 1300, 'max_latency_seconds': 45.2, 'record_count': 0, 'ingestion_time_utc': '2025-10-22T14:15:00Z', 'source': 's3', 'health_status': 'CRITICAL'}, {'pipeline_name': 'product_catalog_sync', 'status_code': 200, 'duration_seconds': 800, 'record_count': 80, 'max_latency_seconds': 15.0, 'errors': [], 'warnings': ['schema mismatch'], 'ingestion_time': '2025-10-08T09:00:00Z', 'source': 'api', 'health_status': 'WARNING'}, {'pipeline_name': 'inventory_update', 'status_code': 200, 'duration_seconds': 300, 'record_count': 1500, 'max_latency_seconds': 8.0, 'errors': [], 'warnings': [], 'ingestion_time': '2025-10-08T03:45:00Z', 'source': 'ftp', 'health_status': 'HEALTHY'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of total pipeline executed and count of each health satuts category\n",
        "\n",
        "healthy_count = 0\n",
        "warning_count = 0\n",
        "critical_count = 0\n",
        "\n",
        "for log in evaluated_logs:\n",
        "    if log[\"health_status\"] == \"HEALTHY\":\n",
        "        healthy_count += 1\n",
        "    elif log[\"health_status\"] == \"WARNING\":\n",
        "        warning_count += 1\n",
        "    elif log[\"health_status\"] == \"CRITICAL\":\n",
        "        critical_count += 1\n",
        "\n",
        "print(f\"Total pipelines evaluated: {len(evaluated_logs)}\")\n",
        "print(f\"HEALTHY: {healthy_count}\")\n",
        "print(f\"WARNING: {warning_count}\")\n",
        "print(f\"CRITICAL: {critical_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8H6t3MTTMvZ",
        "outputId": "0dc42be9-1f5d-4802-f802-cfdab6088dd2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pipelines evaluated: 4\n",
            "HEALTHY: 2\n",
            "WARNING: 1\n",
            "CRITICAL: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bonus Challenge -\n",
        "\n"
      ],
      "metadata": {
        "id": "Gnix-nSElL8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}